{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20653,"status":"ok","timestamp":1714527407770,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"b-V9GocL-hsW","outputId":"e02d3140-b656-4a4b-9e34-7e521b9629bb"},"outputs":[],"source":["import copy\n","import os\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3061,"status":"ok","timestamp":1714527410830,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"30kEJJAR-nNK","outputId":"dc49ab71-ceb5-48bf-bacb-a457bd7d8008"},"outputs":[],"source":["import argparse\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1714527411053,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"0cclPdov-pRG"},"outputs":[],"source":["def get_hyper_parameters():\n","    _para_list = [{\"optim_type\": 'adam', 'lr': 0.0001, \"weight_decay\": 1e-5, \"store_img_training\": True}]\n","    _num_epoch = 40\n","    _patience = 10\n","    _device = 'cuda'\n","    return _para_list, _num_epoch, _patience, _device"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":17822,"status":"ok","timestamp":1714527482389,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"P64ftHVC-qzi"},"outputs":[],"source":["model = StipplingReconstructionCNN(1)\n","para_list, num_epoch, patience, device_str = get_hyper_parameters()\n","# train loader\n","train_dataset = DataLoader_Stippling('./data/median_res/stipple/train', './data/median_res/grayscale/train')\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","\n","# val loader\n","val_dataset = DataLoader_Stippling('./data/median_res/stipple/val', './data/median_res/grayscale/val')\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"elapsed":50982,"status":"error","timestamp":1714033205869,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"26hZTHi0-sFc","outputId":"e4b6e483-2bc6-483d-817c-1bd1743bf1a4"},"outputs":[],"source":["best_model, stats = train_model(model, train_loader, val_loader, num_epoch, para_list[0], patience, device_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDKFVmGIsRHj"},"outputs":[],"source":["save_checkpoint(best_model, 40, \"./checkpoints\", stats)\n","# Plotting\n","plt.figure(figsize=(10, 6)) # Sets the figure size\n","plt.plot(stats['loss_ind'], stats['loss'], marker='o', linestyle='-', color='b')\n","plt.title('Loss over Time') # Title of the plot\n","plt.xlabel('Index') # X-axis label\n","plt.ylabel('Loss') # Y-axis label\n","plt.grid(True) # Shows a grid\n","plt.show() # Displays the plot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":3363,"status":"error","timestamp":1714031604100,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"1284xPC5LYL3","outputId":"0dac3756-f12d-413c-ddab-897323480643"},"outputs":[],"source":["# test loader\n","test_dataset = DataLoader_Stippling('./data/median_res/stipple/test', './data/median_res/grayscale/test')\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n","\n","device = torch.device('cuda')\n","best_model = StipplingReconstructionCNN(1)\n","checkpoint = torch.load(\"./checkpoints/final.pth.tar\")\n","best_model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","\n","best_model.to(device)\n","best_model.eval()\n","\n","print('------------------------ Start Testing ------------------------')\n","\n","i = 0\n","for batch, data in enumerate(test_loader):\n","    print('Batch No. {0}'.format(batch))\n","\n","    X = data['stippling'].to(device)\n","    y = data['grayscale'].to(device)\n","\n","    reconstructed_img = best_model(X)\n","\n","    cv2.imwrite(\"./test_res/\" + str(i) + \"_grayscale.png\", y[0].cpu().squeeze().numpy() * 255.0)\n","    cv2.imwrite(\"./test_res/\" + str(i) + \"_stippling.png\", X[0].cpu().squeeze().numpy() * 255.0)\n","    cv2.imwrite(\"./test_res/\" + str(i) + \"_reconstruction.png\", reconstructed_img[0].cpu().squeeze().detach().numpy() * 255.0)\n","    i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"elapsed":420,"status":"error","timestamp":1712876468573,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"pQnD5DzUSQH9","outputId":"2f36c8fb-05b9-45d1-c35b-ae1c8b7072e9"},"outputs":[],"source":["model, _, _ = restore_checkpoint(model, \"./checkpoints\", cuda=True, force=False, pretrain=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":542,"status":"ok","timestamp":1714527421679,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"w2_UNLw2-t1B"},"outputs":[],"source":["from utils import get_optimizer, reconstruct_image\n","from performance import calculate_loss, get_performance\n","import torch\n","import time\n","import copy\n","import cv2\n","def train_model(model, train_loader, val_loader, num_epoch, parameter, patience, device_str):\n","    device = torch.device(device_str if device_str == 'cuda' and torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.train()\n","\n","    loss_all, loss_index = [], []\n","    num_itration = 0\n","    best_model, best_val_loss = model, float('inf')\n","    num_bad_epoch = 0\n","\n","    optim = get_optimizer(model, parameter[\"optim_type\"], parameter[\"lr\"], parameter[\"weight_decay\"])\n","    scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0, total_iters=num_epoch)\n","\n","    print('------------------------ Start Training ------------------------')\n","    t_start = time.time()\n","    loss = 0\n","    for epoch in range(num_epoch):\n","        for batch, data in enumerate(train_loader):\n","            if (batch % 5 == 0):\n","                print('Batch No. {0}'.format(batch))\n","\n","            X = data['stippling'].to(device)\n","            y = data['grayscale'].to(device)\n","\n","            num_itration += 1\n","\n","            model.train()\n","            optim.zero_grad()\n","            reconstructed_img = model(X)\n","\n","            loss= calculate_loss(reconstructed_img, y)\n","            loss.requires_grad_().backward()\n","            optim.step()\n","\n","            loss_all.append(loss.item())\n","            loss_index.append(num_itration)\n","\n","        print('Epoch No. {0} -- loss = {1:.4f}'.format(\n","            epoch + 1,\n","            loss.item(),\n","        ))\n","\n","        # Validation:\n","        print('Validation')\n","        val_loss = get_performance(model, val_loader, device_str)\n","        model.to(device)\n","        print(\"Validation loss: {:.4f}\".format(val_loss))\n","\n","        if val_loss < best_val_loss:\n","            best_model = copy.deepcopy(model)\n","            best_val_loss = val_loss\n","            num_bad_epoch = 0\n","        else:\n","            num_bad_epoch += 1\n","\n","        # early stopping\n","        if num_bad_epoch >= patience:\n","            break\n","\n","        # learning rate scheduler\n","        scheduler.step()\n","\n","    t_end = time.time()\n","    print('Training lasted {0:.2f} minutes'.format((t_end - t_start) / 60))\n","    print('------------------------ Training Done ------------------------')\n","    stats = {'loss': loss_all,\n","             'loss_ind': loss_index,\n","             }\n","\n","    return best_model, stats"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":198,"status":"ok","timestamp":1714527417950,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"8f3TflRTwKZ7"},"outputs":[],"source":["from torch import nn\n","import numpy as np\n","import torch.optim as optimizer\n","from matplotlib import pyplot as plt\n","import cv2\n","import torch\n","import torch.nn.functional as F\n","from utils import get_optimizer, reconstruct_image\n","\n","def calculate_loss(reconstructed_img, target_img):\n","    loss = F.mse_loss(reconstructed_img, target_img)\n","\n","    return loss\n","\n","def get_performance(model, val_loader, device_str):\n","    device = torch.device(device_str if device_str == 'cuda' and torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    loss_all = []\n","    for _, data in enumerate(val_loader):\n","        X = data['stippling'].to(device)\n","        y = data['grayscale'].to(device)\n","\n","        reconstructed_img = reconstruct_image(X, model, False)\n","\n","        loss= calculate_loss(reconstructed_img, y)\n","\n","        loss_all.append(loss.item())\n","\n","    val_loss = sum(loss_all) / len(loss_all)\n","    return val_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1714527416415,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"2YyQDxRLBHLY"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import cv2\n","import glob\n","\n","class DataLoader_Stippling(Dataset):\n","    def __init__(self, stippling_dir, grayscale_dir):\n","        \"\"\"\n","        Args:\n","            stippling_dir (string): Directory with all the stippling images.\n","            grayscale_dir (string): Directory with all the grayscale images.\n","        \"\"\"\n","        self.stippling_files = sorted(glob.glob(stippling_dir + '/*.png'))\n","        self.grayscale_files = sorted(glob.glob(grayscale_dir + '/*.png'))\n","\n","    def __len__(self):\n","        return len(self.stippling_files)\n","\n","    def __getitem__(self, idx):\n","        # No need to check if idx is a tensor, since we're not using it in a tensor form\n","        stippling_img_path = self.stippling_files[idx]\n","        grayscale_img_path = self.grayscale_files[idx]\n","\n","        # Load images using cv2\n","        stippling_img = torch.FloatTensor(cv2.resize(cv2.imread(stippling_img_path, cv2.IMREAD_GRAYSCALE), (800, 592))).unsqueeze(0) / 255.0\n","        grayscale_img = torch.FloatTensor(cv2.resize(cv2.imread(grayscale_img_path, cv2.IMREAD_GRAYSCALE), (800, 592))).unsqueeze(0) / 255.0\n","\n","\n","        sample = {'stippling': stippling_img, 'grayscale': grayscale_img}\n","\n","        return sample\n","\n","def get_data_loader(stippling_dir, grayscale_dir, batch_size):\n","    custom_dataset = DataLoader_Stippling(stippling_dir, grayscale_dir)\n","    return DataLoader(custom_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":136,"status":"ok","timestamp":1714527459494,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"vyskW09TBIZJ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.nn.functional import relu\n","\n","class StipplingReconstructionCNN(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        # Encoder\n","        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image.\n","        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n","        # -------\n","        # input: 800x600x1\n","        self.e11 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # output: 798x766x64\n","        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 796x764x64\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 398x382x64\n","\n","        # input: 510x382x64\n","        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 508x380x128\n","        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 394x378x128\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 197x189x128\n","\n","        # input: 253x189x128\n","        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 251x138x256\n","        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 193x136x256\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 97x68x256\n","\n","        # input: 68x68x256\n","        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n","        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 93x64x512\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True) # output: 32x32x512\n","\n","        # input: 32x32x512\n","        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n","        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n","\n","\n","        # Decoder\n","        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n","        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n","        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n","        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n","        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","\n","        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n","        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n","        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n","        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        # self.median_blur = MedianBlur(kernel_size=5)\n","        # self.bilateral_filter = BilateralFilter(d=5, sigmaColor=0.5, sigmaSpace=2.0)\n","\n","        # Output layer\n","        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Encoder\n","        # print(x.size())\n","        xe11 = relu(self.e11(x))\n","        # print(xe11.size())\n","        xe12 = relu(self.e12(xe11))\n","        # print(xe12.size())\n","        xp1 = self.pool1(xe12)\n","        # print(xp1.size())\n","\n","        xe21 = relu(self.e21(xp1))\n","        # print(xe21.size())\n","        xe22 = relu(self.e22(xe21))\n","        # print(xe22.size())\n","        xp2 = self.pool2(xe22)\n","        # print(xp2.size())\n","\n","        xe31 = relu(self.e31(xp2))\n","        # print(xe31.size())\n","        xe32 = relu(self.e32(xe31))\n","        # print(xe32.size())\n","        xp3 = self.pool3(xe32)\n","        # print(xp3.size())\n","\n","        xe41 = relu(self.e41(xp3))\n","        # print(xe41.size())\n","        xe42 = relu(self.e42(xe41))\n","        # print(xe42.size())\n","        xp4 = self.pool4(xe42)\n","        # print(xp4.size())\n","\n","        xe51 = relu(self.e51(xp4))\n","        # print(xe51.size())\n","        xe52 = relu(self.e52(xe51))\n","        # print(xe52.size())\n","\n","        # Decoder\n","        xu1 = self.upconv1(xe52)\n","        # print(xu1.size())\n","        xu11 = torch.cat([xu1, xe42], dim=1)\n","        # print(xu11.size())\n","        xd11 = relu(self.d11(xu11))\n","        # print(xd11.size())\n","        xd12 = relu(self.d12(xd11))\n","        # print(xd12.size())\n","\n","        xu2 = self.upconv2(xd12)\n","        xu22 = torch.cat([xu2, xe32], dim=1)\n","        xd21 = relu(self.d21(xu22))\n","        xd22 = relu(self.d22(xd21))\n","\n","        xu3 = self.upconv3(xd22)\n","        xu33 = torch.cat([xu3, xe22], dim=1)\n","        xd31 = relu(self.d31(xu33))\n","        xd32 = relu(self.d32(xd31))\n","\n","        xu4 = self.upconv4(xd32)\n","        xu44 = torch.cat([xu4, xe12], dim=1)\n","        xd41 = relu(self.d41(xu44))\n","        xd42 = relu(self.d42(xd41))\n","\n","\n","\n","        # Output layer\n","        out = self.outconv(xd42)\n","        print(out.size())\n","\n","        return out"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1709,"status":"ok","timestamp":1714527414969,"user":{"displayName":"Chengfan Li","userId":"18313242744225946937"},"user_tz":240},"id":"iNjXi5NnBUR6"},"outputs":[],"source":["import numpy as np\n","import itertools\n","import os\n","import torch\n","from torch.nn.functional import softmax\n","from sklearn import metrics\n","import utils\n","\n","def save_checkpoint(model, epoch, checkpoint_dir, stats):\n","    \"\"\"Save a checkpoint file to `checkpoint_dir`.\"\"\"\n","    state = {\n","        \"epoch\": epoch,\n","        \"state_dict\": model.state_dict(),\n","        \"stats\": stats,\n","    }\n","\n","    filename = os.path.join(checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(epoch))\n","    torch.save(state, filename)\n","\n","def restore_checkpoint(model, checkpoint_dir, cuda=False, force=False, pretrain=False):\n","    \"\"\"Restore model from checkpoint if it exists.\n","\n","    Returns the model and the current epoch.\n","    \"\"\"\n","    try:\n","        cp_files = [\n","            file_\n","            for file_ in os.listdir(checkpoint_dir)\n","            if file_.startswith(\"epoch=\") and file_.endswith(\".checkpoint.pth.tar\")\n","        ]\n","    except FileNotFoundError:\n","        cp_files = None\n","        os.makedirs(checkpoint_dir)\n","    if not cp_files:\n","        print(\"No saved model parameters found\")\n","        if force:\n","            raise Exception(\"Checkpoint not found\")\n","        else:\n","            return model, 0, []\n","\n","    # Find latest epoch\n","    for i in itertools.count(1):\n","        if \"epoch={}.checkpoint.pth.tar\".format(i) in cp_files:\n","            epoch = i\n","        else:\n","            break\n","\n","    if not force:\n","        print(\n","            \"Which epoch to load from? Choose in range [0, {}].\".format(epoch),\n","            \"Enter 0 to train from scratch.\",\n","        )\n","        print(\">> \", end=\"\")\n","        inp_epoch = int(input())\n","        if inp_epoch not in range(epoch + 1):\n","            raise Exception(\"Invalid epoch number\")\n","        if inp_epoch == 0:\n","            print(\"Checkpoint not loaded\")\n","            clear_checkpoint(checkpoint_dir)\n","            return model, 0, []\n","    else:\n","        print(\"Which epoch to load from? Choose in range [1, {}].\".format(epoch))\n","        inp_epoch = int(input())\n","        if inp_epoch not in range(1, epoch + 1):\n","            raise Exception(\"Invalid epoch number\")\n","\n","    filename = os.path.join(\n","        checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(inp_epoch)\n","    )\n","\n","    print(\"Loading from checkpoint {}?\".format(filename))\n","\n","    if cuda:\n","        checkpoint = torch.load(filename)\n","    else:\n","        # Load GPU model on CPU\n","        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n","\n","    try:\n","        start_epoch = checkpoint[\"epoch\"]\n","        stats = checkpoint[\"stats\"]\n","        if pretrain:\n","            model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n","        else:\n","            model.load_state_dict(checkpoint[\"state_dict\"])\n","        print(\n","            \"=> Successfully restored checkpoint (trained for {} epochs)\".format(\n","                checkpoint[\"epoch\"]\n","            )\n","        )\n","    except:\n","        print(\"=> Checkpoint not successfully restored\")\n","        raise\n","\n","    return model, inp_epoch, stats\n","\n","def clear_checkpoint(checkpoint_dir):\n","    \"\"\"Remove checkpoints in `checkpoint_dir`.\"\"\"\n","    filelist = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth.tar\")]\n","    for f in filelist:\n","        os.remove(os.path.join(checkpoint_dir, f))\n","\n","    print(\"Checkpoint successfully removed\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
